{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.281755Z",
     "start_time": "2025-07-04T22:30:12.268541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from seq2seq_translation.datasets.datasets import LanguagePairsDatasets\n",
    "from seq2seq_translation.models.transformer.decoder import DecoderTransformer\n",
    "from seq2seq_translation.sentence_pairs_dataset import SentencePairsDataset\n",
    "from seq2seq_translation.tokenization.sentencepiece_tokenizer import SentencePieceTokenizer\n",
    "from seq2seq_translation.run import _fix_model_state_dict"
   ],
   "id": "effbabb0de09f8be",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "torch.random.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.494364Z",
     "start_time": "2025-07-04T22:30:37.474936Z"
    }
   },
   "id": "5176f06e6aa5f6b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_prefix='/Users/adam.amster/seq2seq_translation/tokenizer/32000/32000',\n",
    "    include_language_tag=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.519771Z",
     "start_time": "2025-07-04T22:30:37.503673Z"
    }
   },
   "id": "5942c27f80686dae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['DEVICE'] = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.550794Z",
     "start_time": "2025-07-04T22:30:37.548775Z"
    }
   },
   "id": "d49393923246e455",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.565245Z",
     "start_time": "2025-07-04T22:30:37.557067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from seq2seq_translation.config.transformer_config import TransformerConfig\n",
    "\n",
    "with open('/Users/adam.amster/Downloads/train_config_decoder.json') as f:\n",
    "    config = json.load(f)\n",
    "config = TransformerConfig.model_validate(config)"
   ],
   "id": "2a121b267e3dd956",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def construct_test_dset():\n",
    "\ttest_datasets = LanguagePairsDatasets(\n",
    "\t\t\tdata_path=Path('/Users/adam.amster/seq2seq_translation/datasets/wmt14_test/wmt___wmt14/fr-en/0.0.0/b199e406369ec1b7634206d3ded5ba45de2fe696/wmt14-test.arrow'),\n",
    "\t\t\tsource_lang='en',\n",
    "\t\t\ttarget_lang='fr',\n",
    "\t\t\tis_test=True\n",
    "\t)\n",
    "\t\n",
    "\ttest_dset = SentencePairsDataset(\n",
    "\t\tdatasets=test_datasets,\n",
    "\t\tidxs=np.arange(len(test_datasets)),\n",
    "\t\tcombined_tokenizer=tokenizer,\n",
    "\t\tmax_length=None,\n",
    "        eos_token_id=tokenizer.eot_idx,\n",
    "        pad_token_id=tokenizer.pad_idx,\n",
    "        combine_source_and_target=True,\n",
    "        source_language_tag_token_id=tokenizer.language_tag_map[config.source_lang],\n",
    "        target_language_tag_token_id=tokenizer.language_tag_map[config.target_lang],\n",
    "\t)\n",
    "\treturn test_dset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:37.574187Z",
     "start_time": "2025-07-04T22:30:37.571678Z"
    }
   },
   "id": "fd372758075226e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "test_dset = construct_test_dset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:38.487285Z",
     "start_time": "2025-07-04T22:30:37.580394Z"
    }
   },
   "id": "92f8ac9a2a42c9fc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "def construct_model():\n",
    "\tmodel = DecoderTransformer(\n",
    "        n_attention_heads=config.n_head,\n",
    "        n_layers=config.num_layers,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=config.d_model,\n",
    "        block_size=config.fixed_length,\n",
    "        feedforward_hidden_dim=config.feedforward_hidden_dim,\n",
    "        norm_first=config.norm_first,\n",
    "        mlp_activation=config.activation,\n",
    "        use_cross_attention=False,\n",
    "        positional_encoding_type=config.positional_encoding_type,\n",
    "        pad_token_idx=tokenizer.pad_idx,\n",
    "\t)\n",
    "\n",
    "\tmodel.load_state_dict(\n",
    "\t\t_fix_model_state_dict(torch.load('/Users/adam.amster/Downloads/ckpt_decoder.pt', map_location='cpu')[\"model\"])\n",
    "\t)\n",
    "\n",
    "\tmodel.eval()\n",
    "\treturn model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:38.498491Z",
     "start_time": "2025-07-04T22:30:38.495962Z"
    }
   },
   "id": "6cdd466198dd3557",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "model = construct_model()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:39.164810Z",
     "start_time": "2025-07-04T22:30:38.508580Z"
    }
   },
   "id": "ad0b0f1e96d6b18",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:39.731061Z",
     "start_time": "2025-07-04T22:30:39.174804Z"
    }
   },
   "cell_type": "code",
   "source": "[i for i,x in enumerate(test_dset) if test_dset._datasets[i][0] == \"The cinema was ventilated and everyone returned in good order.\"]",
   "id": "b1d1b174347e91f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[634]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "long_examples = [x for x in test_dset if len(x[0]) > 19]\n",
    "rand_idxs = [2184, 1956, 1679, 634]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T22:30:40.161063Z",
     "start_time": "2025-07-04T22:30:39.739663Z"
    }
   },
   "id": "878901c00a028194",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def get_attn_weights(examples):\n",
    "    sources = []\n",
    "    preds = []\n",
    "    attn_weights = []\n",
    "    for ex in examples:\n",
    "        source = ex[0].unsqueeze(0)\n",
    "        target = ex[1].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred, attention_weights = model.generate(\n",
    "                x=source,\n",
    "                top_k=1,\n",
    "                return_attention_weights=True,\n",
    "                eot_token_id=tokenizer.eot_idx,\n",
    "                pad_token_id=tokenizer.pad_idx,\n",
    "                include_input=True,\n",
    "            )\n",
    "            print(f'source: {tokenizer.decode(source[0])}')\n",
    "            print(f'target: {tokenizer.decode(target[0])}')\n",
    "            print(f'pred: {tokenizer.decode(pred)}')\n",
    "        sources.append(source)\n",
    "        preds.append(pred)\n",
    "        attn_weights.append(attention_weights)\n",
    "    return preds, attn_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T23:26:11.855364Z",
     "start_time": "2025-07-04T23:26:11.850919Z"
    }
   },
   "id": "36f8da6ab595c013",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:57:51.668257Z",
     "start_time": "2025-07-05T12:57:37.480671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math, numpy as np, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ───────────  STYLE CONSTANTS  ───────────\n",
    "ROW_H_PX   = 22          # px between token rows\n",
    "FONT_SIZE  = 14          # pt\n",
    "COL_GAP    = 1.8         # x-distance between columns\n",
    "LINE_ALPHA = 0.45        # line opacity\n",
    "LW_SCALE   = 4           # max line width multiplier\n",
    "THRESH     = 0.1        # skip weights < THRESH\n",
    "# ─────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ─────────────────────  ONE HEAD  →  ONE SUBPLOT  ──────────────────────\n",
    "def add_arc_subplot(fig, row, col,\n",
    "                    weights, left_tok, right_tok,\n",
    "                    *, show=True):\n",
    "    \"\"\"\n",
    "    Draw vertical/vertical arc diagram for a single head in subplot (row, col).\n",
    "\n",
    "    * first token in each list is at the TOP row (y = 0)\n",
    "    * one blue line per weight ≥ THRESH\n",
    "    * shorter column simply ends – no phantom lines\n",
    "    \"\"\"\n",
    "    T_q, T_k  = weights.shape\n",
    "    max_len   = max(T_q, T_k)\n",
    "    y_left    = np.arange(T_q) * ROW_H_PX\n",
    "    y_right   = np.arange(T_k) * ROW_H_PX\n",
    "    ymax      = (max_len - 1) * ROW_H_PX + ROW_H_PX / 2\n",
    "\n",
    "    # 1️⃣ lines under the text\n",
    "    for i in range(T_q):\n",
    "        for j in range(T_k):\n",
    "            w = float(weights[i, j])\n",
    "            if w < THRESH:\n",
    "                continue\n",
    "            # coordinates for a *dense* line -----------------------------\n",
    "            n_pts   = 20                                  # ← tweak if needed\n",
    "            xs      = np.linspace(0.15, COL_GAP - 0.15, n_pts)\n",
    "            ys      = np.linspace(y_left[i], y_right[j], n_pts)\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=xs, y=ys,\n",
    "\n",
    "                    # draw the blue segment\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(width=LW_SCALE * w,\n",
    "                              color=f'rgba(65,105,225,{LINE_ALPHA})'),\n",
    "\n",
    "                    # invisible markers sitting *on* the line\n",
    "                    marker=dict(size=8, color='rgba(0,0,0,0)'),\n",
    "\n",
    "                    # one tooltip for all points\n",
    "                    hoverinfo='text',\n",
    "                    hovertext=[(\n",
    "                        f\"src: {left_tok[i]}\"\n",
    "                        f\"<br>tgt: {right_tok[j]}\"\n",
    "                        f\"<br>weight: {w:.3f}\"\n",
    "                    )] * n_pts,\n",
    "                    hovertemplate=\"%{hovertext}<extra></extra>\",\n",
    "\n",
    "                    showlegend=False,\n",
    "                    visible=show),\n",
    "                row=row, col=col)\n",
    "\n",
    "\n",
    "    # 2️⃣ token labels over the lines\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.zeros(T_q), y=y_left,\n",
    "                   mode='text', text=left_tok,\n",
    "                   textfont=dict(size=FONT_SIZE, color='black'),\n",
    "                   textposition='middle right',\n",
    "                   showlegend=False, visible=show),\n",
    "        row=row, col=col)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.full(T_k, COL_GAP), y=y_right,\n",
    "                   mode='text', text=right_tok,\n",
    "                   textfont=dict(size=FONT_SIZE, color='black'),\n",
    "                   textposition='middle left',\n",
    "                   showlegend=False, visible=show),\n",
    "        row=row, col=col)\n",
    "\n",
    "    # 3️⃣ axes\n",
    "    fig.update_xaxes(visible=False, range=[-0.5, COL_GAP + .5], row=row, col=col)\n",
    "    fig.update_yaxes(visible=False, range=[-ROW_H_PX / 2, ymax],\n",
    "                     autorange='reversed', row=row, col=col)\n",
    "\n",
    "\n",
    "# ─────────────────────────────  DRIVER  ────────────────────────────────\n",
    "def plot_attn_weights(examples, base_output_dir: Path, n_columns: int = 3):\n",
    "    \"\"\"\n",
    "    Build full multi-head / multi-layer arc-diagram figure and write to HTML+JSON.\n",
    "    \"\"\"\n",
    "    preds, attn = get_attn_weights(examples=examples)\n",
    "\n",
    "    for ex in range(1):\n",
    "        L = len(attn[ex][0])              # layers\n",
    "        H = attn[ex][0][0].shape[1]       # heads\n",
    "        n_rows = math.ceil(H / n_columns)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_columns,\n",
    "            horizontal_spacing=.09, vertical_spacing=.03,\n",
    "            subplot_titles=[f'Head {h+1}' for h in range(H)])\n",
    "\n",
    "        left = []\n",
    "        for x in preds[ex][0]:\n",
    "            if x == tokenizer.language_tag_map[config.source_lang]:\n",
    "                left.append(f'&lt;{config.source_lang}&gt;')\n",
    "            elif x == tokenizer.language_tag_map[config.target_lang]:\n",
    "                left.append(f'&lt;{config.target_lang}&gt;')\n",
    "            else:\n",
    "                tok = tokenizer.processor.id_to_piece(x.item())\n",
    "                # HTML-escape <s>, </s>\n",
    "                if tok == '<s>':\n",
    "                    tok = '&lt;s&gt;'\n",
    "                elif tok == '</s>':\n",
    "                    tok = '&lt;/s&gt;'\n",
    "                left.append(tok)\n",
    "        right = left\n",
    "\n",
    "        # build all subplots & store trace ids per layer\n",
    "        traces_by_layer = [[] for _ in range(L)]\n",
    "\n",
    "        for lay in range(L):\n",
    "            for hd in range(H):\n",
    "                r = hd // n_columns + 1\n",
    "                c = hd %  n_columns + 1\n",
    "                visible = (lay == 0)\n",
    "                start = len(fig.data)\n",
    "\n",
    "                #print(ex, lay, hd, start)\n",
    "                add_arc_subplot(\n",
    "                    fig, r, c,\n",
    "                    weights=attn[ex][0][lay][0, hd],\n",
    "                    left_tok=left,\n",
    "                    right_tok=right,\n",
    "                    show=visible)\n",
    "\n",
    "                traces_by_layer[lay].extend(range(start, len(fig.data)))\n",
    "\n",
    "        # slider\n",
    "        steps = []\n",
    "        for lay, ids in enumerate(traces_by_layer):\n",
    "            vis = [False]*len(fig.data)\n",
    "            for i in ids: vis[i] = True\n",
    "            steps.append(dict(label=f'Layer {lay+1}',\n",
    "                              method='update',\n",
    "                              args=[{'visible': vis}]))\n",
    "\n",
    "        max_tok   = max(len(left), len(right))\n",
    "        subplot_h = max_tok * ROW_H_PX + 40\n",
    "\n",
    "        fig.update_layout(\n",
    "            sliders=[dict(active=0, steps=steps,\n",
    "                          pad={'t':60,'b':10},\n",
    "                          currentvalue=dict(font=dict(size=FONT_SIZE+1,\n",
    "                                                      color='black')))],\n",
    "            autosize=False,\n",
    "            width  = n_columns * 320,\n",
    "            height = n_rows * subplot_h + 120,\n",
    "            margin=dict(t=20,l=20,r=20,b=60),\n",
    "            font=dict(size=FONT_SIZE, color='black'),\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "        )\n",
    "        for ann in fig.layout.annotations:\n",
    "            ann.font.update(size=FONT_SIZE+2, color='black')\n",
    "\n",
    "        base = (f'{base_output_dir}/'\n",
    "                f'multi_head_decoder_attention_weights_{ex}')\n",
    "        fig.write_json(base+'.json')\n",
    "        fig.write_html(base+'.html')\n",
    "plot_attn_weights(examples=[test_dset[idx] for idx in rand_idxs], base_output_dir=Path('/Users/adam.amster/aamster.github.io/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/'))"
   ],
   "id": "c31ed3e9db7cb43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: But why such optimism for some and pessimism for others?\n",
      "target: Les raisons d'un tel optimisme, chez les uns, et pessimisme, chez les autres ?\n",
      "pred: But why such optimism for some and pessimism for others? Mais pourquoi un tel optimisme pour certains et un tel pessimisme pour les autres?\n",
      "source: Regulatory authority over phone calls belongs to the Federal Communications Commission, not the FAA.\n",
      "target: Le pouvoir réglementaire concernant les téléphones portables appartient à la Federal Communications Commission et non à la FAA.\n",
      "pred: Regulatory authority over phone calls belongs to the Federal Communications Commission, not the FAA. L'autorité réglementaire sur les appels téléphoniques appartient à la Commission fédérale des communications et non à la LGFP.\n",
      "source: They don't want us to dictate to them what makes them profitable.\n",
      "target: Elles ne veulent pas qu'on leur dise ce qui leur permettra d'être rentables.\n",
      "pred: They don't want us to dictate to them what makes them profitable. Ils ne veulent pas que nous leur dictions ce qui les rend rentables.\n",
      "source: The cinema was ventilated and everyone returned in good order.\n",
      "target: La salle a été aérée et tout est rentré dans l'ordre.\n",
      "pred: The cinema was ventilated and everyone returned in good order. Le cinéma était ventilé et tout le monde est revenu en bon ordre.\n"
     ]
    }
   ],
   "execution_count": 82
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
