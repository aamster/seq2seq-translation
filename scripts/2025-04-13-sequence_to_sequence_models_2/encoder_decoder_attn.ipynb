{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:02.349999Z",
     "start_time": "2025-06-23T01:20:02.346293Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from seq2seq_translation.datasets.datasets import LanguagePairsDatasets\n",
    "from seq2seq_translation.sentence_pairs_dataset import SentencePairsDataset\n",
    "from seq2seq_translation.tokenization.sentencepiece_tokenizer import SentencePieceTokenizer\n",
    "from seq2seq_translation.run import _fix_model_state_dict"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "torch.random.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:02.366618Z",
     "start_time": "2025-06-23T01:20:02.363007Z"
    }
   },
   "id": "5176f06e6aa5f6b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "source_tokenizer = SentencePieceTokenizer(model_prefix='/Users/adam.amster/seq2seq_translation/tokenizer/30000/en')\n",
    "target_tokenizer = SentencePieceTokenizer(model_prefix='/Users/adam.amster/seq2seq_translation/tokenizer/30000/fr')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:02.412322Z",
     "start_time": "2025-06-23T01:20:02.378045Z"
    }
   },
   "id": "5942c27f80686dae",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['DEVICE'] = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:02.423218Z",
     "start_time": "2025-06-23T01:20:02.421176Z"
    }
   },
   "id": "d49393923246e455",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "def construct_test_dset():\n",
    "\ttest_datasets = LanguagePairsDatasets(\n",
    "\t\t\tout_dir=Path('/Users/adam.amster/seq2seq_translation/datasets/wmt14_test'),\n",
    "\t\t\tsource_lang='en',\n",
    "\t\t\ttarget_lang='fr',\n",
    "\t\t\tis_test=True\n",
    "\t)\n",
    "\t\n",
    "\ttest_dset = SentencePairsDataset(\n",
    "\t\tdatasets=test_datasets,\n",
    "\t\tidxs=np.arange(len(test_datasets)),\n",
    "\t\tsource_tokenizer=source_tokenizer,\n",
    "\t\ttarget_tokenizer=target_tokenizer,\n",
    "\t\tmax_length=None,\n",
    "        eos_token_id=source_tokenizer.eot_idx,\n",
    "        pad_token_id=source_tokenizer.pad_idx,\n",
    "\t)\n",
    "\treturn test_dset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:02.435099Z",
     "start_time": "2025-06-23T01:20:02.432779Z"
    }
   },
   "id": "fd372758075226e6",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "test_dset = construct_test_dset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:07.901384Z",
     "start_time": "2025-06-23T01:20:02.445114Z"
    }
   },
   "id": "92f8ac9a2a42c9fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4e9b562f2a84b9b8351f16e60d675cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "from seq2seq_translation.config.transformer_config import TransformerConfig\n",
    "import json\n",
    "from seq2seq_translation.models.transformer.encoder_decoder import EncoderDecoderTransformer\n",
    "\n",
    "\n",
    "def construct_model():\n",
    "\twith open('/Users/adam.amster/Downloads/train_config-2.json') as f:\n",
    "\t\tconfig = json.load(f)\n",
    "\tconfig = TransformerConfig.model_validate(config)\n",
    "\n",
    "\tmodel = EncoderDecoderTransformer(\n",
    "\t\tn_attention_heads=config.n_head,\n",
    "\t\tn_layers=config.num_layers,\n",
    "\t\tvocab_size=source_tokenizer.vocab_size,\n",
    "\t\td_model=config.d_model,\n",
    "\t\tblock_size=config.max_input_length,\n",
    "\t\tfeedforward_hidden_dim=config.feedforward_hidden_dim,\n",
    "\t\tsos_token_id=source_tokenizer.processor.bos_id(),\n",
    "\t\teos_token_id=source_tokenizer.processor.eos_id(),\n",
    "\t\tpad_token_id=source_tokenizer.processor.pad_id(),\n",
    "\t\tnorm_first=config.norm_first,\n",
    "\t\tmlp_activation=config.activation,\n",
    "\t\tpositional_encoding_type=config.positional_encoding_type,\n",
    "\t)\n",
    "\n",
    "\tmodel.load_state_dict(\n",
    "\t\t_fix_model_state_dict(torch.load('/Users/adam.amster/Downloads/ckpt.pt', map_location='cpu')[\"model\"])\n",
    "\t)\n",
    "\n",
    "\tmodel.eval()\n",
    "\treturn model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:07.927743Z",
     "start_time": "2025-06-23T01:20:07.922797Z"
    }
   },
   "id": "6cdd466198dd3557",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": "encoder_decoder = construct_model()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:09.013925Z",
     "start_time": "2025-06-23T01:20:07.939841Z"
    }
   },
   "id": "ad0b0f1e96d6b18",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "def get_test_dset_examples():\n",
    "\tshort = [x for x in test_dset if  9 <= len(x[0]) <= 19]\n",
    "\treturn short"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:09.027380Z",
     "start_time": "2025-06-23T01:20:09.025229Z"
    }
   },
   "id": "f153cd66edc93261",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "test_set = get_test_dset_examples()\n",
    "long_examples = [x for x in test_dset if len(x[0]) > 19]\n",
    "rand_idxs = [592, 533, 463, 187]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-23T01:20:09.796247Z",
     "start_time": "2025-06-23T01:20:09.038960Z"
    }
   },
   "id": "878901c00a028194",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:13:07.120736Z",
     "start_time": "2025-06-23T11:13:07.083696Z"
    }
   },
   "cell_type": "code",
   "source": "target_tokenizer.decode(test_dset[302][1])",
   "id": "a2b91a4cefb9aa83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chaque constructeur planifie de façon différente.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:56:16.460267Z",
     "start_time": "2025-06-18T15:56:16.458072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class AttnType(Enum):\n",
    "    ENCODER_SELF = 'ENCODER_SELF'\n",
    "    DECODER_SELF = 'DECODER_SELF'\n",
    "    DECODER_CROSS = 'DECODER_CROSS'"
   ],
   "id": "f6519b2cbd3b0a69",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def get_attn_weights(attention_type: AttnType, examples):\n",
    "\tsources = []\n",
    "\tpreds = []\n",
    "\tattn_weights = []\n",
    "\tfor ex in examples:\n",
    "\t\tsource = ex[0].unsqueeze(0)\n",
    "\t\ttarget = ex[1].unsqueeze(0)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tif attention_type == AttnType.ENCODER_SELF:\n",
    "\t\t\t\tattention_weights = encoder_decoder.encoder(source, src_key_padding_mask=torch.ones_like(source, dtype=torch.bool),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn_attention_weights=True)\n",
    "\t\t\telif attention_type == AttnType.DECODER_CROSS:\n",
    "\t\t\t\t_, _, attention_weights = encoder_decoder.generate(\n",
    "\t\t\t\t\tsrc=source,\n",
    "\t\t\t\t\ttop_k=1,\n",
    "                    return_cross_attention_weights=True\n",
    "\t\t\t\t)\n",
    "\t\t\tpred, _ = encoder_decoder.generate(\n",
    "\t\t\t\tsrc=source,\n",
    "\t\t\t\ttop_k=1,\n",
    "\t\t\t)\n",
    "\t\t\tprint(f'source: {source_tokenizer.decode(source[0])}')\n",
    "\t\t\tprint(f'target: {target_tokenizer.decode(target[0])}')\n",
    "\t\t\tprint(f'pred: {target_tokenizer.decode(pred)}')\n",
    "\t\tsources.append(source)\n",
    "\t\tpreds.append(pred)\n",
    "\t\tattn_weights.append(attention_weights)\n",
    "\treturn sources, preds, attn_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T00:12:28.745086Z",
     "start_time": "2025-06-22T00:12:28.737914Z"
    }
   },
   "id": "36f8da6ab595c013",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T00:12:20.568165Z",
     "start_time": "2025-06-22T00:12:20.546641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math, numpy as np, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from enum import Enum\n",
    "\n",
    "# ───────────  STYLE CONSTANTS  ───────────\n",
    "ROW_H_PX   = 22          # px between token rows\n",
    "FONT_SIZE  = 14          # pt\n",
    "COL_GAP    = 1.8         # x-distance between columns\n",
    "LINE_ALPHA = 0.45        # line opacity\n",
    "LW_SCALE   = 4           # max line width multiplier\n",
    "THRESH     = 0.02        # skip weights < THRESH\n",
    "# ─────────────────────────────────────────\n",
    "\n",
    "class AttnType(Enum):\n",
    "    ENCODER_SELF  = 1\n",
    "    DECODER_SELF  = 2\n",
    "    DECODER_CROSS = 3\n",
    "\n",
    "\n",
    "# ─────────────────────  ONE HEAD  →  ONE SUBPLOT  ──────────────────────\n",
    "def add_arc_subplot(fig, row, col,\n",
    "                    weights, left_tok, right_tok,\n",
    "                    *, show=True):\n",
    "    \"\"\"\n",
    "    Draw vertical/vertical arc diagram for a single head in subplot (row, col).\n",
    "\n",
    "    * first token in each list is at the TOP row (y = 0)\n",
    "    * one blue line per weight ≥ THRESH\n",
    "    * shorter column simply ends – no phantom lines\n",
    "    \"\"\"\n",
    "    T_q, T_k  = weights.shape\n",
    "    max_len   = max(T_q, T_k)\n",
    "    y_left    = np.arange(T_q) * ROW_H_PX\n",
    "    y_right   = np.arange(T_k) * ROW_H_PX\n",
    "    ymax      = (max_len - 1) * ROW_H_PX + ROW_H_PX / 2\n",
    "\n",
    "    # 1️⃣ lines under the text\n",
    "    for i in range(T_q):\n",
    "        for j in range(T_k):\n",
    "            w = float(weights[i, j])\n",
    "            if w < THRESH:\n",
    "                continue\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[0.15, COL_GAP - 0.15],\n",
    "                    y=[y_left[i], y_right[j]],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=LW_SCALE * w,\n",
    "                              color=f'rgba(65,105,225,{LINE_ALPHA})'),\n",
    "                    hoverinfo='none',\n",
    "                    showlegend=False,\n",
    "                    visible=show),\n",
    "                row=row, col=col)\n",
    "\n",
    "    # 2️⃣ token labels over the lines\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.zeros(T_q), y=y_left,\n",
    "                   mode='text', text=left_tok,\n",
    "                   textfont=dict(size=FONT_SIZE, color='black'),\n",
    "                   textposition='middle right',\n",
    "                   showlegend=False, visible=show),\n",
    "        row=row, col=col)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.full(T_k, COL_GAP), y=y_right,\n",
    "                   mode='text', text=right_tok,\n",
    "                   textfont=dict(size=FONT_SIZE, color='black'),\n",
    "                   textposition='middle left',\n",
    "                   showlegend=False, visible=show),\n",
    "        row=row, col=col)\n",
    "\n",
    "    # 3️⃣ axes\n",
    "    fig.update_xaxes(visible=False, range=[-0.5, COL_GAP + .5], row=row, col=col)\n",
    "    fig.update_yaxes(visible=False, range=[-ROW_H_PX / 2, ymax],\n",
    "                     autorange='reversed', row=row, col=col)\n",
    "\n",
    "\n",
    "# ─────────────────────────────  DRIVER  ────────────────────────────────\n",
    "def plot_attn_weights(examples, base_output_dir: Path, attn_type: AttnType, n_columns: int = 3):\n",
    "    \"\"\"\n",
    "    Build full multi-head / multi-layer arc-diagram figure and write to HTML+JSON.\n",
    "    \"\"\"\n",
    "    sources, preds, attn = get_attn_weights(attention_type=attn_type, examples=examples)\n",
    "\n",
    "    for ex in range(1):\n",
    "        L = len(attn[ex])              # layers\n",
    "        H = attn[ex][0].shape[1]       # heads\n",
    "        n_rows = math.ceil(H / n_columns)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_columns,\n",
    "            horizontal_spacing=.09, vertical_spacing=.03,\n",
    "            subplot_titles=[f'Head {h+1}' for h in range(H)])\n",
    "\n",
    "        # token lists for this sentence\n",
    "        if attn_type == AttnType.ENCODER_SELF:\n",
    "            left  = right = [source_tokenizer.processor.id_to_piece(x.item())\n",
    "                             for x in sources[ex][0]]\n",
    "        elif attn_type == AttnType.DECODER_SELF:\n",
    "            left  = right = [target_tokenizer.processor.id_to_piece(x.item())\n",
    "                             for x in preds[ex][0]]\n",
    "        else:  # DECODER_CROSS\n",
    "            left  = [target_tokenizer.processor.id_to_piece(x.item())\n",
    "                     for x in preds[ex][0]]\n",
    "            right = [source_tokenizer.processor.id_to_piece(x.item())\n",
    "                     for x in sources[ex][0]]\n",
    "\n",
    "        # HTML-escape <s>, </s>\n",
    "        esc = lambda t: '&lt;s&gt;' if t == '<s>' else ('&lt;/s&gt;' if t == '</s>' else t)\n",
    "        left  = list(map(esc, left))\n",
    "        right = list(map(esc, right))\n",
    "\n",
    "        # build all subplots & store trace ids per layer\n",
    "        traces_by_layer = [[] for _ in range(L)]\n",
    "\n",
    "        for lay in range(L):\n",
    "            for hd in range(H):\n",
    "                r = hd // n_columns + 1\n",
    "                c = hd %  n_columns + 1\n",
    "                visible = (lay == 0)\n",
    "                start = len(fig.data)\n",
    "\n",
    "                add_arc_subplot(\n",
    "                    fig, r, c,\n",
    "                    weights=attn[ex][lay][0, hd],\n",
    "                    left_tok=left,\n",
    "                    right_tok=right,\n",
    "                    show=visible)\n",
    "\n",
    "                traces_by_layer[lay].extend(range(start, len(fig.data)))\n",
    "\n",
    "        # slider\n",
    "        steps = []\n",
    "        for lay, ids in enumerate(traces_by_layer):\n",
    "            vis = [False]*len(fig.data)\n",
    "            for i in ids: vis[i] = True\n",
    "            steps.append(dict(label=f'Layer {lay+1}',\n",
    "                              method='update',\n",
    "                              args=[{'visible': vis}]))\n",
    "\n",
    "        max_tok   = max(len(left), len(right))\n",
    "        subplot_h = max_tok * ROW_H_PX + 40\n",
    "\n",
    "        fig.update_layout(\n",
    "            sliders=[dict(active=0, steps=steps,\n",
    "                          pad={'t':60,'b':10},\n",
    "                          currentvalue=dict(font=dict(size=FONT_SIZE+1,\n",
    "                                                      color='black')))],\n",
    "            autosize=False,\n",
    "            width  = n_columns * 320,\n",
    "            height = n_rows * subplot_h + 120,\n",
    "            margin=dict(t=20,l=20,r=20,b=60),\n",
    "            font=dict(size=FONT_SIZE, color='black'),\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "        )\n",
    "        for ann in fig.layout.annotations:\n",
    "            ann.font.update(size=FONT_SIZE+2, color='black')\n",
    "\n",
    "        tag = {AttnType.ENCODER_SELF:  'encoder_self',\n",
    "               AttnType.DECODER_SELF:  'decoder_self',\n",
    "               AttnType.DECODER_CROSS: 'decoder_cross'}[attn_type]\n",
    "        base = (f'{base_output_dir}/'\n",
    "                f'multi_head_{tag}_attention_weights_{ex}')\n",
    "        fig.write_json(base+'.json')\n",
    "        fig.write_html(base+'.html')"
   ],
   "id": "c31ed3e9db7cb43",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:43:55.663997Z",
     "start_time": "2025-06-20T16:43:51.529478Z"
    }
   },
   "cell_type": "code",
   "source": "plot_attn_weights(attn_type=AttnType.ENCODER_SELF, examples=[test_set[idx] for idx in rand_idxs], base_output_dir=Path('/Users/adam.amster/aamster.github.io/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/'))",
   "id": "88e613a8aa05014d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: But why such optimism for some and pessimism for others?\n",
      "target: Les raisons d'un tel optimisme, chez les uns, et pessimisme, chez les autres ?\n",
      "pred: Mais pourquoi un tel optimisme pour certains et le pessimisme pour d'autres?\n",
      "source: Regulatory authority over phone calls belongs to the Federal Communications Commission, not the FAA.\n",
      "target: Le pouvoir réglementaire concernant les téléphones portables appartient à la Federal Communications Commission et non à la FAA.\n",
      "pred: Le pouvoir de réglementation des appels téléphoniques appartient à la Commission fédérale des communications, et non à la FAA.\n",
      "source: They don't want us to dictate to them what makes them profitable.\n",
      "target: Elles ne veulent pas qu'on leur dise ce qui leur permettra d'être rentables.\n",
      "pred: Ils ne veulent pas que nous leur dictions ce qui les rend rentables.\n",
      "source: The cinema was ventilated and everyone returned in good order.\n",
      "target: La salle a été aérée et tout est rentré dans l'ordre.\n",
      "pred: Le cinéma a été ventilé et tout le monde est retourné en bon état.\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:07:40.753060Z",
     "start_time": "2025-06-20T12:07:32.168331Z"
    }
   },
   "cell_type": "code",
   "source": "plot_attn_weights(attn_type=AttnType.DECODER_CROSS, examples=[test_set[idx] for idx in rand_idxs], base_output_dir=Path('/Users/adam.amster/aamster.github.io/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/'))",
   "id": "66f5e518c551aaac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: But why such optimism for some and pessimism for others?\n",
      "target: Les raisons d'un tel optimisme, chez les uns, et pessimisme, chez les autres ?\n",
      "pred: Mais pourquoi un tel optimisme pour certains et le pessimisme pour d'autres?\n",
      "source: Regulatory authority over phone calls belongs to the Federal Communications Commission, not the FAA.\n",
      "target: Le pouvoir réglementaire concernant les téléphones portables appartient à la Federal Communications Commission et non à la FAA.\n",
      "pred: Le pouvoir de réglementation des appels téléphoniques appartient à la Commission fédérale des communications, et non à la FAA.\n",
      "source: They don't want us to dictate to them what makes them profitable.\n",
      "target: Elles ne veulent pas qu'on leur dise ce qui leur permettra d'être rentables.\n",
      "pred: Ils ne veulent pas que nous leur dictions ce qui les rend rentables.\n",
      "source: The cinema was ventilated and everyone returned in good order.\n",
      "target: La salle a été aérée et tout est rentré dans l'ordre.\n",
      "pred: Le cinéma a été ventilé et tout le monde est retourné en bon état.\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T00:24:10.464497Z",
     "start_time": "2025-06-22T00:24:02.228399Z"
    }
   },
   "cell_type": "code",
   "source": "plot_attn_weights(attn_type=AttnType.ENCODER_SELF, examples=[long_examples[np.random.choice(len(long_examples))]], base_output_dir=Path('/tmp'))",
   "id": "efa14d26c13aeb86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: While Murphy said the purpose of the trip is to help improve relationships, he said some \"tough love\" will also be dispensed.\n",
      "target: Murphy a déclaré que le but de ce voyage était de permettre d'améliorer les relations, mais également de faire preuve de « fermeté affectueuse ».\n",
      "pred: Bien que le but du voyage soit d'améliorer les relations, il a dit que l'on dispensera aussi certains « amours très serrés ».\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
